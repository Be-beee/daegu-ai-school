# -*- coding: utf-8 -*-
"""02-2-Supervised-learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oO00oj2OJFW63lcp7pTnsrURf28F2r7V
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn

import mglearn

"""# Supervised Machine Learning Algorithms"""

X, y = mglearn.datasets.make_forge()

mglearn.discrete_scatter(X[:,0], X[:,1], y)
plt.legend(['Class 0','Class 1'])
plt.xlabel('First feature')
plt.ylabel('Second feature')
print('X.shape:', X.shape)

X, y = mglearn.datasets.make_wave(n_samples=40)

plt.plot(X, y, 'o')
plt.ylim(-3, 3)
plt.xlabel('Feature')
plt.ylabel('Target')

# 유방암 종양의 임상 데이터 위스콘신(Wisconsin Breast Cancer)

from sklearn.datasets import load_breast_cancer

cancer = load_breast_cancer()
print('cancer.keys():', cancer.keys())

print('Shape of cancer data:', cancer.data.shape)

print('Sample counts per class:\n',
      {n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))})

"""### References

- [train_test_split 모듈을 활용하여 학습과 테스트 세트 분리](https://teddylee777.github.io/scikit-learn/train-test-split)
- [구조의 재배열, numpy.reshape 함수](https://yganalyst.github.io/data_handling/memo_5/)
- [내장 함수 zip 사용법](https://www.daleseo.com/python-zip/)
- [bincount](https://nurilee.com/2020/05/10/bincount-%EB%9E%80/)
"""

cancer.target_names

print('Feature names:\n', cancer.feature_names)

# 보스턴 주택 가격

from sklearn.datasets import load_boston

boston = load_boston()
print('Data shape:', boston.data.shape)

boston['feature_names']

"""## k-Nearest Neighbors

k-Neighbors classification
"""

mglearn.plots.plot_knn_classification(n_neighbors=1)

mglearn.plots.plot_knn_classification(n_neighbors=3)

X, y = mglearn.datasets.make_forge()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

from sklearn.neighbors import KNeighborsClassifier

clf = KNeighborsClassifier(n_neighbors=3) # hyperparameter

clf.fit(X_train, y_train)

print('Test set predictions:', clf.predict(X_test))

print('Test set accuracy:', clf.score(X_test, y_test))

"""## Analyzing KNeighborsClassifier"""

fig, axes = plt.subplots(1, 3, figsize=(10, 3))
for n_neighbors, ax in zip([1, 3, 9], axes):
    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)
    
    mglearn.plots.plot_2d_separator(clf, X, fill=True, ax=ax, alpha=0.4)
    mglearn.discrete_scatter(X[:,0], X[:,1], y, ax=ax)

from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=66)

test = load_breast_cancer()

training_accuracy = []
test_accuracy = []

neighbors_settings = range(1, 11)

for n_neighbors in neighbors_settings:
    print(n_neighbors)
    clf = KNeighborsClassifier(n_neighbors=n_neighbors)
    clf.fit(X_train, y_train)
    
    # record training set accuracy
    training_accuracy.append(clf.score(X_train, y_train))
    
    # record test set accuracy
    test_accuracy.append(clf.score(X_test, y_test))

plt.plot(neighbors_settings, training_accuracy, label='training accuracy')
plt.plot(neighbors_settings, test_accuracy, label='test accuracy') # 주목
plt.xlabel('n_neighbors')
plt.ylabel('accuracy')
plt.legend()

"""## k-neighbors regression"""

mglearn.plots.plot_knn_regression(n_neighbors=1)

mglearn.plots.plot_knn_regression(n_neighbors=3)

from sklearn.neighbors import KNeighborsRegressor

X, y = mglearn.datasets.make_wave(n_samples=40)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
reg = KNeighborsRegressor(n_neighbors=3)
reg.fit(X_train, y_train)

reg.predict(X_test)

reg.score(X_test, y_test)

X.shape #Reshape: 형태 바꾸기

"""[Reshape<br>https://yganalyst.github.io/data_handling/memo_5/](https://yganalyst.github.io/data_handling/memo_5/)

# Analyzing KNeighborsRegressor
"""

fig, axes = plt.subplots(1,3, figsize=(15,4))
line = np.linspace(-3, 3, 1000).reshape(-1, 1)

for n_neighbors, ax in zip([1, 3, 9], axes):
    reg = KNeighborsRegressor(n_neighbors=n_neighbors)
    reg.fit(X_train, y_train)
    ax.plot(line, reg.predict(line))
    
    ax.plot(X_train, y_train, '^', markersize=8)
    ax.plot(X_test, y_test, 'v', markersize=8)
    
    ax.set_title('{} neighbor(s)\n train score:{:.2f} test socre:{:.2f}'.format(
                n_neighbors, reg.score(X_train, y_train), reg.score(X_test, y_test)))
    
    ax.set_xlabel('feature')
    ax.set_ylabel('target')
    ax.legend(['Model predictions', 'Training data/target', 'Test data/target'])

"""# Linear Models"""

mglearn.plots.plot_linear_regression_wave()

"""[scikit-learn](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)

![scikit-learn](https://scikit-learn.org/stable/_static/ml_map.png)
"""

from sklearn.linear_model import LinearRegression

X, y = mglearn.datasets.make_wave(n_samples=60)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

lr = LinearRegression()

lr.fit(X_train, y_train)

print('Training set score:', lr.score(X_train, y_train))
print('Test set score:', lr.score(X_test, y_test))

X, y = mglearn.datasets.load_extended_boston()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

lr = LinearRegression()
lr.fit(X_train, y_train)

print('Training set score:', lr.score(X_train, y_train))
print('Test set score:', lr.score(X_test, y_test))

"""#### Ridge regression 리지 회귀
리지 회귀는 과대접합이 되지 않도록 모델을 강제로 제한한다.
linear_model.Ridge에 구현되어 있다.
"""

from sklearn.linear_model import Ridge

ridge = Ridge()

ridge.fit(X_train, y_train)

print('Training set score:', ridge.score(X_train, y_train))
print('Test set score:', ridge.score(X_test, y_test))

ridge10 = Ridge(alpha=10)
ridge10.fit(X_train, y_train)

print('Training set score:', ridge10.score(X_train, y_train))
print('Test set score:', ridge10.score(X_test, y_test))

ridge01 = Ridge(alpha=0.1)
ridge01.fit(X_train, y_train)

print('Training set score: {:.2f}'.format(ridge01.score(X_train, y_train)))
print('Test set score:{:.2f}'.format(ridge01.score(X_test, y_test)))

"""## 데이터의 시각화"""

plt.plot(ridge.coef_, 's', label='Ridge alpha=1')
plt.plot(ridge10.coef_, '^', label='Ridge alpha=10')
plt.plot(ridge01.coef_, 'v', label='Ridge alpha=0.1')
plt.legend(['ridge alpha=1', 'ridge alpha=10', 'ridge alpha=0.1'])

mglearn.plots.plot_ridge_n_samples() # ridge alpha = 1 기준

"""#### Lasso 라소
라소는 Ridge의 대안으로 제시되며 특정 항목들은 제한이 0이 되면서 특성을 선택할 수 있게 해준다. 이는 모델의 특성을 노출하는데 도움이 된다.
"""

from sklearn.linear_model import Lasso

lasso = Lasso()

lasso.fit(X_train, y_train)

print('Training set score:', lasso.score(X_train, y_train))
print('Test set score:', lasso.score(X_test, y_test))
print('Number of features used:', np.sum(lasso.coef_ != 0))

X.shape

lasso001 = Lasso(alpha = 0.01, max_iter=10000)

lasso001.fit(X_train, y_train)

print('Training set score:', lasso001.score(X_train, y_train))
print('Test set score:', lasso001.score(X_test, y_test))
print('Number of features used:', np.sum(lasso001.coef_ != 0))

lasso00001 = Lasso(alpha = 0.0001, max_iter=100000) # 제한 거의 없음
lasso00001.fit(X_train, y_train)

print('Training set score:', lasso00001.score(X_train, y_train)) # 과대적합
print('Test set score:', lasso00001.score(X_test, y_test))
print('Number of features used:', np.sum(lasso00001.coef_ != 0))

plt.plot(lasso.coef_, 's', label='Lasso alpha=1')
plt.plot(lasso001.coef_, '^', label='Lasso alpha=0.01')
plt.plot(lasso00001.coef_, 'v', label='Lasso alpha=0.0001')

"""## Linear models for classification"""

mglearn.plots.plot_linear_svc_regularization()

from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC # (SVC(Support Vector Classifier))

X, y = mglearn.datasets.make_forge()

fig, axes = plt.subplots(1,2,figsize=(10, 3))
for model, ax in zip([LinearSVC(), LogisticRegression()], axes):
    clf = model.fit(X, y)
    mglearn.plots.plot_2d_separator(clf, X, ax=ax)
    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)
    ax.set_title(clf.__class__.__name__)

mglearn.plots.plot_linear_svc_regularization()

from sklearn.datasets import load_breast_cancer

cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)

logreg = LogisticRegression()
logreg.fit(X_train, y_train)

print('Training set score:', logreg.score(X_train, y_train))
print('Test set score:', logreg.score(X_test, y_test))

logreg100 = LogisticRegression(C=100).fit(X_train, y_train)

print('Training set score:', logreg100.score(X_train, y_train))
print('Test set score:', logreg100.score(X_test, y_test))

logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)
print('Training set score:', logreg001.score(X_train, y_train))
print('Test set score:', logreg001.score(X_test, y_test))

plt.plot(logreg.coef_.T, 'o', label="C=1")
plt.plot(logreg100.coef_.T, '^', label="C=100")
plt.plot(logreg001.coef_.T, 'v', label="C=0.001")
plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)
xlims = plt.xlim()
plt.hlines(0, xlims[0], xlims[1])
plt.xlim(xlims)
plt.ylim(-5, 5)
plt.xlabel("Feature")
plt.ylabel("Coefficient magnitude")
plt.legend()

logreg.coef_.T

cancer.feature_names

"""### 제약의 종류
- L1: **Feature에 대한 제약**을 이야기합니다. (Max 반복값이 지정되어야 함) (ex. Lasso Regression)
- L2: **C는 학습량에 대한 제약**을 이야기합니다. (ex. Ridge Regression)
"""

for C, marker in zip([0.001, 1, 100], ['o','^','v']):
    lr_l1 = LogisticRegression(C=C, solver='liblinear', penalty='l1').fit(X_train, y_train)
    
    print('Trainig accuracy of l1 logreg with C={:.3f} : {:.2f}'.format(
    C, lr_l1.score(X_train, y_train)))
    print('Test accuracy of l1 logreg with C={:.3f} : {:.2f}'.format(
    C, lr_l1.score(X_test, y_test)))
    print('-----------------------------------------------------')
    plt.plot(lr_l1.coef_.T, marker, label='C={:.3f}'.format(C))
    
plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)
xlims = plt.xlim()
plt.hlines(0, xlims[0], xlims[1])
plt.xlim(xlims)
plt.xlabel("Feature")
plt.ylabel("Coefficient magnitude")

plt.ylim(-5, 5)
plt.legend(loc=3)

"""[가상데이터 생성 방법](https://datascienceschool.net/03%20machine%20learning/09.02%20%EB%B6%84%EB%A5%98%EC%9A%A9%20%EA%B0%80%EC%83%81%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%83%9D%EC%84%B1.html)"""

from sklearn.datasets import make_blobs

X, y = make_blobs(random_state=42)
mglearn.discrete_scatter(X[:,0], X[:,1], y)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
plt.legend(['Class 0', 'Class 1', 'Class 2'])

linear_svm = LinearSVC().fit(X, y)
print(linear_svm.coef_.shape)
print(linear_svm.intercept_.shape)

linear_svm.coef_

mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
line = np.linspace(-15, 15)

for coef, intercept in zip(linear_svm.coef_, linear_svm.intercept_):
    plt.plot(line, -(line * coef[0] + intercept) / coef[1])

mglearn.plots.plot_2d_classification(linear_svm, X, alpha=0.7)
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
line = np.linspace(-15, 15)

for coef, intercept in zip(linear_svm.coef_, linear_svm.intercept_):
    plt.plot(line, -(line * coef[0] + intercept) / coef[1])
    
plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1', 'Line class 2'])
plt.xlabel('Feature 0')
plt.ylabel('Feature 1')

